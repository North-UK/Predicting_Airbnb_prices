# Discussion

Overall, the RF model appears to offer satisfactory results both in terms of predictive accuracy and in measures of fit, explaining a 55% in the variation, compared to 53 in the OLS model. If we follow the rule of thumb, an R2 of 0.55 is to be considerate a ‘moderate’ result. It is useful to remember that the model is attempting to predict what humans would be willing to pay for an accommodation after all. And predicting human behaviour is always a difficult thing. Yet, improvements could be made. 
RF have become very popular learning algorithms, with good enough predictive performances that require relatively small parameters tunings. This became obvious when the first algorithm was implemented, before tuning took place. The OOB error achieved after the hyperparameter tuning did not seem to improve in any significant way. This raises the question whether the extra computational power required to tune the random forest pays off.  
From here on, it appears that, where we adopt the params suggested by the grid search where OBB is minimised, the chances of improving model performance reduce drastically. A quick try in further tunning the parameters show no improvement, revealing the grid to be best option. This brings us to perhaps the biggest limitation within the random forests, namely its black box nature. Consisting of a large number of trees and having each of those trees trained on bagged data, gaining enough insight into the model to understand why it produced the results becomes infeasible. This raises yet further questions on whether we should be considering the trade-offs between predictability and interpretability. 

## Possible ways forward

Although a further change in the hyperparameters seems to make no difference to the model’s predictive power, predictability might increase if we were to introduce new variables such as ‘transit’ or ‘access’ into the model. Some data pre-processing and cleaning would need to be done, which may take considerable time. 
Another variable that could benefit the predictive power would be the Zip code. Considering the dataset contains thousands of codes, using the first part (BL5 or OL11) of the zip code would suffice.
A possible strategy could be recursive elimination of variables, where we first compute RF variable importance then eliminate the 20% pf the variables that come out with having the least importance and proceed by building a new model with the remaining variables. Finally, we select the variables that lead to the smallest OOB.
In summary, considering increasing the number of variables seems to be the surest way to increase predictive power.
