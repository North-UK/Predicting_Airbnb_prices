[["index.html", "Predicting Airbnb Prices with a Random Forest model Chapter 1 Executive Summary", " Predicting Airbnb Prices with a Random Forest model Henrique Moura 2021-09-23 Chapter 1 Executive Summary This study aimed at constructing a Random Forest model for AirBnb pricing prediction using a bymber of factors related to rental data. The work is divided into an introduction, where the problem and the study aims are described. Methods in section 2 describes the data and the RF model and any data pre-processing. In section 3 and 4, the tuning and results are addressed and a critical evaluation of the model carried out, respectively. "],["intro.html", "Chapter 2 Introduction 2.1 Study Aims", " Chapter 2 Introduction Established in 2007, Airbnb is an American company that operates an online marketplace for private lodging, with over five million active listings in 220 countries and regions. Hosts rely solely on private listings, with little or no support provided by the platform when it comes to pricing. A high price could mean a loss in possible costumers, and a price that is set too low might translate into a loss of profits. In turn, this project attempts to address this issue by developing a predictive Random Forest (RF) model of Airbnb rental pricing based on a variety of factors including reviews and bedrooms. 2.1 Study Aims Working closely with a client, this project attempts to develop a Random Forest Model (RF) to predict appropriate pricing for their portfolio of potential listings. 2.1.1 Random Forest Described first by Breiman (Breiman 2001), Random forest is an ensemble learning method, operating by the construction of a multitude of decision trees and being capable of handling both classification and regression type problems. The logic behind the model is that multiple uncorrelated decisions trees perform significantly better as a group than they do alone. Each tree comprises a data sample from the training set with replacement, called the bootstrap sample. Randomness is then injected through feature bagging, and a vote (i.e.Â the most frequent categorical variable) yields the predicted class. The most sought after advantaged of the algorithm is its default ability to overcome tree correlation trough the introduction of randomness in the tree building process and thus being less prone to over fitting. Another key feature of the Random Forest model, is its ability to select and focus on the most important features. Yet, due to its black box nature, gaining enough insight into the model to understand why it produced the results becomes a difficult if nopt impossible feat. References "],["methods.html", "Chapter 3 Methods 3.1 Data pre-processing 3.2 Building the Model", " Chapter 3 Methods Data for this project can be accessed through the open platform Inside Airbnb. Although the listings data provides us with much to work with, some cleaning and transforming still needs to be carried out in order for the model to be built. 3.1 Data pre-processing The training data contains 4886 observations with a total of 9 features, log_price being the dependent, or target variable, and the others the independent, or predictors. The data are messy, needing to be cleaned and transformed, with new variables needing to be created. The following data preprocessing actions are thus taken: ## Price: # Data that describes prices in $s are converted to numbers. The distribution of the variable as seen below is skewed, so we log the rental prices. listings$price = dollar_to_number(listings$price) Figure 3.1: The raw Airbnb prices. Figure 3.2: The log of the Airbnb price data. # Listings with price value of zero are removed: listings=listings[listings$price&gt;0,] ## Categorical variables, such as those that describe the type of property and the type of room, are reduced to fewer categories, and in this case binary variables are used (i.e., 1 or 0). The same is done for the cleaning fee variable. listings$property_type_House = (listings$property_type == &quot;House&quot;)+0 listings$property_type_Other = (listings$property_type == &quot;Other&quot;)+0 listings$room_type_Private_room = (listings$room_type == &quot;Private room&quot;)+0 listings$room_type_Shared_room = (listings$room_type == &quot;Shared room&quot;)+0 listings$cleaning_fee[is.na(listings$cleaning_fee)] = 0 ## we fill any empty gaps by applying the median value: listings$bathrooms[is.na(listings$bathrooms)]=median(listings$bathrooms, na.rm=T) listings$beds[is.na(listings$beds)]= median(listings$beds, na.rm=T) As a result, the we are left with the following variables: Variable Type Description log_price numeric Night price for rental. accomodates integer Number of people the room accommodates. beds integer Number of bedrooms. bathrooms numeric No pre-processing was necessary for this variable. It is a measure of how many people the property accommodates. cleaning_fee numeric Whether a cleaning fee exists or not. property_type_House numeric Type of Property. property_type_Other numeric Type of Property - Other than house. room_type_Private_room numeric Type of Room. room_type_Share_room numeric Type of Room - Other. The head of the preprocessed data can be seen below: ## # A tibble: 6 x 9 ## log_price accommodates beds bathrooms cleaning_fee property_type_House ## &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 4.17 4 6 1 1 1 ## 2 4.09 2 1 1.5 0 0 ## 3 3.53 3 2 1 1 0 ## 4 4.01 2 2 1 0 1 ## 5 6.86 2 1 1 0 0 ## 6 4.09 2 1 1 0 1 ## # ... with 3 more variables: property_type_Other &lt;dbl&gt;, ## # room_type_Private_room &lt;dbl&gt;, room_type_Shared_room &lt;dbl&gt; Before creating the model, the createDatapartition tool is used to split the data into two subsets, one for the model and one to evaluate the models performance. The function splits the data 70/30 respectively. # Splitting the data using the createDatapartition tool: training_index=createDataPartition(data_anal$log_price, p=0.7, list=F) train_x=data_anal[training_index,] test_x=data_anal[-training_index,] Distribution of Categorical Variables Figure 3.3: Distributions of categorical variables Continuous Variables Figure 3.4: Distributions of contious variables 3.2 Building the Model The Random Forest model will be constructed using the data provided by Airbnb, with the price (log_price) as the the targeted variable and the remaining 8 as predictors. reg.mod=log_price~ accommodates + beds + bathrooms + cleaning_fee + property_type_House + property_type_Other + room_type_Private_room + room_type_Shared_room A random forest is a simple predictor that consists of a collection of M randomised trees. M may be chosen arbitrarily and is usually only limited by available computing resources. Since trees are trained independent of each other, with no issue of overfitting (Breiman (2001)), from a modelling point of view it might make sense to let M tend to infinity. But as instructed, we set m at ntree =5000 using the randomForest package. rf1 &lt;- randomForest( formula=reg.mod, ntree=5000, data=train_x ) References "],["tuning-and-results.html", "Chapter 4 Tuning and Results 4.1 Results", " Chapter 4 Tuning and Results The model works by drawing random observations from the original dataset prior to the construction of each tree. A split is then performed over mtry directions which are chosen at random. In turn, Parameters such as mtry, sample size and node size can be tuned to improve performance if needed. Tunning is an important process in model building. As Breiman (Breiman (2001)) points out, the model must aim for an optimal compromise between low correlation while maintaining reasonable predictive strength. Hyperparameter Definition Values mtry Number of drawn candidate variables in each split p/3 for regression (, where p is the number of variables in the dataset). Sample Size Number of observations that are drawn for each tree N (n being the number of observations). Node Size Minimum number of observations in a terminal node 5 for regression . The best parameters can be defined by grid search techniques. In turn, a tuning grid is defined as a first step towards obtaining a more fine-grained control over the parameters in the model: params &lt;- expand.grid( mtry= c(2:8), node_size = seq(3, 15, by = 2), samp_size = c(.65, 0.7, 0.8, 0.9, 1) ) After the tuning grid has been defined, a loop is set up as a means to pass each combination of parameters to the algorithm. The results of each iteration of the loop are saved into a vector. for(i in 1:nrow(params)){ rf.i &lt;- ranger( formula= reg.mod, data= train_x, num.trees= 5000, mtry= params$mtry[i], min.node.size= params$node_size[i], sample.fraction= params$samp_size[i], seed=123 ) # add OOB error to rf.grid rf.grid &lt;- c(rf.grid, sqrt(rf.i$prediction.error)) # print to see progress if (i%%10 == 0) cat(i, &quot;\\t&quot;) } The which.min function extracts the best performing combination of parameters, which in this case are mtry=6, Node Size=3, Samp Size=0.9, and are then applied to a final model: rfFit= ranger( formula = reg.mod, data=train_x, num.trees = 5000, mtry = 6, min.node.size=3, sample.fraction =0.9, seed=123, importance = &quot;impurity&quot; ) We are also able to plot the the variables which contributed most to the model: Figure 4.1: Relative importance of each variable 4.1 Results The results reveal that with the selected parameters, the final model explains about 55% of the variation. There is no standard guideline in determining an acceptable predictive level for R2. A rule of thumb proposed is that an R2 with 0.75, 0.50 and 0.25 can be described as substantial, moderate, and weak, respectively. But this could also be influenced by the context of the research. RMSE Rsquared MAE 0.49 0.55 0.36 To compare results, we build an OLS model: reg.mod = as.formula(log_price ~ accommodates + beds + bathrooms + cleaning_fee + property_type_House + property_type_Other + room_type_Private_room + room_type_Shared_room) m = lm(reg.mod, data = data_anal) summary(m) ## ## Call: ## lm(formula = reg.mod, data = data_anal) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.0207 -0.3083 -0.0609 0.2217 5.0743 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.906290 0.025383 153.895 &lt; 2e-16 *** ## accommodates 0.114073 0.005815 19.616 &lt; 2e-16 *** ## beds 0.009721 0.007659 1.269 0.204405 ## bathrooms 0.069179 0.013832 5.001 5.89e-07 *** ## cleaning_fee -0.077549 0.016110 -4.814 1.53e-06 *** ## property_type_House -0.163889 0.018433 -8.891 &lt; 2e-16 *** ## property_type_Other 0.069984 0.020292 3.449 0.000568 *** ## room_type_Private_room -0.578816 0.019369 -29.883 &lt; 2e-16 *** ## room_type_Shared_room -0.748410 0.074809 -10.004 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5183 on 4837 degrees of freedom ## Multiple R-squared: 0.533, Adjusted R-squared: 0.5322 ## F-statistic: 689.9 on 8 and 4837 DF, p-value: &lt; 2.2e-16 And finally, we can compare how both models predicted the prices for the potential listings: ID Price RF_Price 1 $80 $80 2 $40 $35 3 $40 $35 4 $40 $35 5 $110 $95 6 $65 $70 7 $105 $100 8 $30 $30 9 $65 $70 10 $90 $95 References "],["discussion.html", "Chapter 5 Discussion 5.1 Possible ways forward", " Chapter 5 Discussion Overall, the RF model appears to offer satisfactory results both in terms of predictive accuracy and in measures of fit, explaining a 55% in the variation, compared to 53 in the OLS model. If we follow the rule of thumb, an R2 of 0.55 is to be considerate a moderate result. It is useful to remember that the model is attempting to predict what humans would be willing to pay for an accommodation after all. And predicting human behaviour is always a difficult thing. Yet, improvements could be made. RF have become very popular learning algorithms, with good enough predictive performances that require relatively small parameters tunings. This became obvious when the first algorithm was implemented, before tuning took place. The OOB error achieved after the hyperparameter tuning did not seem to improve in any significant way. This raises the question whether the extra computational power required to tune the random forest pays off. From here on, it appears that, where we adopt the params suggested by the grid search where OBB is minimised, the chances of improving model performance reduce drastically. A quick try in further tunning the parameters show no improvement, revealing the grid to be best option. This brings us to perhaps the biggest limitation within the random forests, namely its black box nature. Consisting of a large number of trees and having each of those trees trained on bagged data, gaining enough insight into the model to understand why it produced the results becomes infeasible. This raises yet further questions on whether we should be considering the trade-offs between predictability and interpretability. 5.1 Possible ways forward Although a further change in the hyperparameters seems to make no difference to the models predictive power, predictability might increase if we were to introduce new variables such as transit or access into the model. Some data pre-processing and cleaning would need to be done, which may take considerable time. Another variable that could benefit the predictive power would be the Zip code. Considering the dataset contains thousands of codes, using the first part (BL5 or OL11) of the zip code would suffice. A possible strategy could be recursive elimination of variables, where we first compute RF variable importance then eliminate the 20% pf the variables that come out with having the least importance and proceed by building a new model with the remaining variables. Finally, we select the variables that lead to the smallest OOB. In summary, considering increasing the number of variables seems to be the surest way to increase predictive power. "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
